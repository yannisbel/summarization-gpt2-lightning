{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import openai\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Annotating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pairwise(prompt, summaries, api_key, model=\"GPT-3.5turbo\"):\n",
    "    \"\"\"\n",
    "    Evaluates and ranks summaries using GPT-3.5turbo API by comparing each pairwise.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The prompt text.\n",
    "    - summaries (list of str): List of summaries to be ranked.\n",
    "    - api_key (str): OpenAI API key.\n",
    "    - model (str): GPT model to use.\n",
    "\n",
    "    Returns:\n",
    "    - pairwise_comparisons (dict): Dictionary with pairwise comparisons.\n",
    "    \"\"\"\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    # Dictionary to store pairwise comparisons\n",
    "    pairwise_comparisons = {}\n",
    "\n",
    "    num_summaries = len(summaries)\n",
    "    \n",
    "    # Compare each pair of summaries\n",
    "    for i in range(num_summaries):\n",
    "        for j in range(i + 1, num_summaries):\n",
    "            summary1 = summaries[i]\n",
    "            summary2 = summaries[j]\n",
    "            \n",
    "            query = f\"Prompt: {prompt}\\n\\nCompare the following summaries and rank them from 1 (better) to 2 (worse):\\n\\n1. {summary1}\\n2. {summary2}\\n\\nRank:\"\n",
    "            \n",
    "            response = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=query,\n",
    "                max_tokens=10,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.0\n",
    "            )\n",
    "            \n",
    "            # Extract the ranking from the response\n",
    "            response_text = response.choices[0].text.strip()\n",
    "            ranking = int(response_text)\n",
    "            \n",
    "            # Store the result as (summary1, summary2) -> ranking\n",
    "            pairwise_comparisons[(i, j)] = ranking\n",
    "\n",
    "    return pairwise_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rankings(pairwise_comparisons, num_summaries):\n",
    "    \"\"\"\n",
    "    Aggregates pairwise comparisons into final rankings.\n",
    "\n",
    "    Parameters:\n",
    "    - pairwise_comparisons (dict): Dictionary with pairwise comparisons.\n",
    "    - num_summaries (int): Number of summaries.\n",
    "\n",
    "    Returns:\n",
    "    - final_ranking (list of int): List of final rankings (1-based index).\n",
    "    \"\"\"\n",
    "    # Initialize ranking scores\n",
    "    scores = np.zeros(num_summaries)\n",
    "    \n",
    "    # Calculate scores based on pairwise comparisons\n",
    "    for (i, j), ranking in pairwise_comparisons.items():\n",
    "        if ranking == 1:\n",
    "            scores[i] += 1\n",
    "        else:\n",
    "            scores[j] += 1\n",
    "    \n",
    "    # Rank summaries based on scores\n",
    "    final_ranking = np.argsort(-scores) + 1\n",
    "\n",
    "    return final_ranking.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Loading the annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotatedSummaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    class AnnotatedSummaryDataset\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (dataset): Dataset containing article and summary.\n",
    "    - tokenizer (transformers): tokenizer used.\n",
    "\n",
    "    Returns:\n",
    "    - inputs (torch): encoded prompt and summary.\n",
    "    - label (int): ranking of the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = item['article']\n",
    "        summaries = item['summary']\n",
    "        rankings = item['rankings']\n",
    "\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        for summary, rank in zip(summaries, rankings):\n",
    "            input_text = f\"[CLS] {prompt} [SEP] {summary} [SEP]\"\n",
    "            inputs.append(self.tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt'))\n",
    "            labels.append(rank - 1)\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'prompt': 'Article text CNN...',\n",
    "        'summaries': ['summary 1', 'summary 2'],\n",
    "        'rankings': [1, 2]\n",
    "    },\n",
    "]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = AnnotatedSummaryDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1 - Building the BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSummaryRanker(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    class BERTSummaryRanker: Initialize the Reward model\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Name of the model to initialize.\n",
    "    - lr (float): learning rate lr.\n",
    "    - num_labels (int): number of labels per data\n",
    "\n",
    "    Returns:\n",
    "    - inputs (torch): encoded prompt and summary.\n",
    "    - label (int): ranking of the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='bert-base-uncased', lr=5e-5, num_labels=2):\n",
    "        super(BERTSummaryRanker, self).__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        input_ids = torch.cat([item['input_ids'] for item in inputs], dim=0).squeeze()\n",
    "        attention_mask = torch.cat([item['attention_mask'] for item in inputs], dim=0).squeeze()\n",
    "        token_type_ids = torch.cat([item['token_type_ids'] for item in inputs], dim=0).squeeze()\n",
    "        labels = torch.tensor(labels).flatten()\n",
    "\n",
    "        outputs = self(input_ids, attention_mask, token_type_ids)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = BERTSummaryRanker(model_name='bert-base-uncased', lr=5e-5)\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator='gpu' if torch.cuda.is_available() else 'cpu')\n",
    "trainer.fit(ranker, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.2 - Building the Classic Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class AnnotatedSummaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    class AnnotatedSummaryDataset\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (dataset): Dataset containing article and summary.\n",
    "    - tokenizer (transformers): tokenizer used.\n",
    "\n",
    "    Returns:\n",
    "    - inputs (dict): dictionnary containing input_ids, attention_mask, token_type_ids.\n",
    "    - labels (torch): ranking of the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = item['prompt']\n",
    "        summaries = item['summaries']\n",
    "        rankings = item['rankings']\n",
    "\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        for summary, rank in zip(summaries, rankings):\n",
    "            input_text = f\"{prompt} [SEP] {summary}\"\n",
    "            encoded_input = self.tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            inputs.append(encoded_input)\n",
    "            labels.append(rank - 1)\n",
    "\n",
    "        input_ids = torch.stack([inp['input_ids'].squeeze() for inp in inputs])\n",
    "        attention_mask = torch.stack([inp['attention_mask'].squeeze() for inp in inputs])\n",
    "        token_type_ids = torch.stack([inp['token_type_ids'].squeeze() for inp in inputs])\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'prompt': 'Article text CNN...',\n",
    "        'summaries': ['summary 1', 'summary 2'],\n",
    "        \"rankings\": [1, 2]\n",
    "    },\n",
    "]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = AnnotatedSummaryDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from trl import RewardTrainer\n",
    "\n",
    "class RewardModel(BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    class RewardModel: Initialize the Reward model\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Name of the model to initialize.\n",
    "    - num_labels (int): number of labels per data\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=4):\n",
    "        super().__init__(config=BertForSequenceClassification.from_pretrained(model_name).config)\n",
    "        self.num_labels = num_labels\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        \"\"\"\n",
    "        def forward: Infer the model with an input\n",
    "\n",
    "        Parameters:\n",
    "        - input_ids (torch): torch containing input_ids, \n",
    "        - attention_mask (torch): torch containing attention_mask,\n",
    "        - token_type_ids (torch): torch containing token_type_ids,\n",
    "        - labels (torch): rankings of the data.\n",
    "\n",
    "        Returns:\n",
    "        - outputs (int): label for the data\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RewardModel(model_name='bert-base-uncased', num_labels=4)\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args={\n",
    "        'output_dir': './results',\n",
    "        'num_train_epochs': 3,\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'per_device_eval_batch_size': 2,\n",
    "        'warmup_steps': 10,\n",
    "        'weight_decay': 0.01,\n",
    "        'logging_dir': './logs',\n",
    "        'logging_steps': 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
